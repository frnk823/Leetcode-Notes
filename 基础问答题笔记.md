# 基础问答题笔记

https://www.cnblogs.com/zhang-qc/p/9410862.html 常见的网络面试题
https://www.nowcoder.com/discuss/271862?type=2 这个更全一点

## 计算机网络
- **TCP三次握手、四次挥手**

- **为什么是三次握手，两次可不可以？**
  因为保证可靠的最小传输的次数就是三次握手，A能发，B能收发，A能收，保证可靠，两次握手显然不可以，不仅不能保证可靠传输，可能导致失效的连接请求被服务端接收，A开始请求无效，后来重发请求成功和B通信，B后来又会和失效的A连接。
  
- **TCP如何保证可靠性?**

- **TIME_WAIT是什么阶段的？如果存在多个tcp请求处在这个阶段，会占用什么资源？有什么后果？**
  在进行关闭连接四次挥手协议时，最后的ACK是由主动关闭端发出的，如果这个最终的ACK丢失，服务器将重发最终的FIN，因此客户端必须维护状态信息允许它重发最终的ACK，这个等待时间就是TIME_WAIT。TIME_WAIT一般是2MSL（最长报文寿命），也就是4分钟。
  在**高并发短连接**的TCP服务器上，当服务器处理完请求后立刻主动正常关闭连接。这个场景下会出现大量socket处于TIME_WAIT状态。由于业务处理时间可能会远小于TIME_WAIT的时间，**如果客户端的并发量持续很高，会占用大量的端口资源，此时部分客户端就会显示连接不上，服务器因端口资源不足而拒绝为一部分客户服务**。
  解决办法简单来说，就是打开系统的TIMEWAIT重用和快速回收，例如复用TIMEWAIT的连接，或者减少TIMEWAIT的大小，开启快速回收。也可以通过负载均衡来对抗这些高并发的短请求，这样可以避免修改timewait造成的其他问题。（说个题外话，从这个意义出发来考虑服务器性能调优的话，长连接业务的服务就不需要考虑TIMEWAIT状态。同时，假如你对服务器业务场景非常熟悉，你会发现，在实际业务场景中，一般长连接对应的业务的并发量并不会很高。）
  https://www.cnblogs.com/dadonggg/p/8778318.html 参考的博客，写得还行
  
- **怎么解决SYN泛洪攻击？**
  
  设置timout时间，屏蔽同ip的多次请求等
  
- **为什么建立连接是三次握手，关闭连接确是四次挥手呢？**
  建立连接的时候， 服务器在LISTEN状态下，收到建立连接请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。 
  而关闭连接时，服务器收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据，而自己也未必全部数据都发送给对方了，所以己方可以立即关闭，也可以发送一些数据给对方后，再发送FIN报文给对方来表示同意现在关闭连接，因此，己方ACK和FIN一般都会分开发送，从而导致多了一次。
  
- **为什么TCP客户端最后还要发送一次确认呢？**
  一句话，主要防止已经失效的连接请求报文突然又传送到了服务器，从而产生错误。
  如果使用的是两次握手建立连接，假设有这样一种场景，客户端发送了第一个请求连接并且没有丢失，只是因为在网络结点中滞留的时间太长了，由于TCP的客户端迟迟没有收到确认报文，以为服务器没有收到，此时重新向服务器发送这条报文，此后客户端和服务器经过两次握手完成连接，传输数据，然后关闭连接。此时此前滞留的那一次请求连接，网络通畅了到达了服务器，这个报文本该是失效的，但是，两次握手的机制将会让客户端和服务器再次建立连接，这将导致不必要的错误和资源的浪费。
  如果采用的是三次握手，就算是那一次失效的报文传送过来了，服务端接受到了那条失效报文并且回复了确认报文，但是客户端不会再次发出确认。由于服务器收不到确认，就知道客户端并没有请求连接。
  
- **如果已经建立了连接，但是客户端突然出现故障了怎么办？**
  TCP还设有一个保活计时器，显然，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75分钟发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。
  
- **TCP长连接和短连接**
  短连接：建立连接——数据传输——关闭连接...建立连接——数据传输——关闭连接
  长连接：建立连接——数据传输...（保持连接）...数据传输——关闭连接
  而像WEB网站的http服务一般都用短链接，因为长连接对于服务端来说会耗费一定的资源，而像WEB网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源，如果用长连接，而且同时有成千上万的用户，如果每个用户都占用一个连接的话，那可想而知吧。所以并发量大，但每个用户无需频繁操作情况下需用短连好。
  
- https://blog.csdn.net/u014465934/article/details/89202797

- **TCP和UDP的区别**
  1.TCP 是面向连接的传输层协议，应用程序在使用 TCP 协议之前，必须先建立 TCP 连接，传输数据完后，需要释放 TCP 连接。UDP 是无连接的，发送数据之前不需要建立连接，因此减少了开销和发送数据的时延。
  2.2.每一条 TCP 连接只能有两个端点，所以 TCP 连接是点对点的。UDP 支持一对一，一对多，多对一和多对多的相互通信。
  3.TCP 提供可靠交互的服务，通过 TCP 连接传送的数据，无差错，不丢失，不重复，按序到达。UDP 使用尽最大努力交付，即不保证交付可靠。
  4.TCP 是面向字节流的，TCP 把应用程序交下来的数据看成仅仅是一连串无结构的字节流，TCP 不关心把多长的报文发送到缓存中，而是根据对方给出的窗口和网络拥塞程度来决定一个报文段应包含多少个字节。UDP 是面向报文的，UDP 对应用层交下来的报文既不合并也不拆分，一次交付一个完整的报文。
  5.TCP 有拥塞控制，UDP 没有拥塞控制，很多实时应用（IP电话，视频会议等），要求主机恒定速率发送数据，并且允许在拥塞的时候丢失数据，所以很适合 UDP。
  6.UDP 首部开销小，只有8个字节。TCP 首部 20 个字节。
  7.TCP 提供全双工通信，允许通信双方在任何时候发送数据，TCP 连接两端都有发送缓存和接收缓存。
<img src="pics/TCP.png">
  
- **TCP的可靠传输的工作原理**
a.停止等待协议/超时重传
  停止等待就是每发完一个分组，就停止发送，等待对方的确认，在收到确认后再发送下一个分组。
  如果超过一段时间没有收到确认，就认为刚才发送的分组丢失了，因而重传刚发发送的分组，这就叫做超时重传。
  超时重传时间应当比数据在分组传输的平均往返时间更长一点。
  上述这种可靠传输协议被称为自动重传请求 ARQ（Automatic Repeat-reQuest）。
b.流水线传输
  停止等待协议的优点是简单，但是信道利用率太低了。解决方法是采用连续ARQ协议，流水线传输使用的就是连续 ARQ 协议和滑动窗口协议。
  首先，位于发送窗口内的分组都可以连续的发送出去，而不需要等待对方的确认。当所有的分组都发送出去以后，根据连续 ARQ 协议规定，发送方每接收到一个确认，就可以把发送窗口向前移动一个分组的位置。
  接收方一般都是采用累积确认的方式。也就是说接收方不必对收到的分组逐个发送确认。而是在收到几个分组后，对按序到达的最后一个分组发送确认（前面的小组都要检查，但是只是对所有OK的分组中最后一个返回确认，这样就可以节省时间了）。如果收到了这个分组确认信息，则表示到这个分组为止的所有分组都已经正确接收到了。
  规则：（1）凡是已经发送过的数据，在未收到确认之前，都必须暂时保留，以便在超时重传时使用。（2）只有当发送方A收到了接收方的确认报文段时，发送方窗口才可以向前滑动几个序号。（3）当发送方A发送的数据经过一段时间没有收到确认（由超时计时器控制），就要使用回退N步协议，回到最后接收到确认号的地方，重新发送这部分数据。
  
- **TCP的流量控制**
  原理这就是运用TCP报文段中的窗口大小字段来控制，发送方的发送窗口不可以大于接收方发回的窗口大小。
  TCP为每一个连接设有一个持续计时器(persistence timer)。只要TCP连接的一方收到对方的零窗口通知，就启动持续计时器。若持续计时器设置的时间到期，就发送一个零窗口控测报文段（携1字节的数据），那么收到这个报文段的一方就重新设置持续计时器。
  
- **TCP的拥塞控制**
  拥塞控制的四种算法，即慢开始（Slow-start)，拥塞避免（Congestion Avoidance)，快重传（Fast Restrangsmit)和快回复（Fast Recovery）
  发送方维持一个拥塞窗口 cwnd ( congestion window )的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。发送方让自己的发送窗口等于拥塞。
  发送方控制拥塞窗口的原则是：只要网络没有出现拥塞，拥塞窗口就再增大一些，以便把更多的分组发送出去。但只要网络出现拥塞，拥塞窗口就减小一些，以减少注入到网络中的分组数。
  **慢开始算法**：即**由小到大逐渐增大发送窗口**，也就是说，由小到大逐渐增大拥塞窗口数值。通常在刚刚开始发送报文段时，先把拥塞窗口 cwnd 设置为一个最大报文段MSS的数值。而在每收到一个对新的报文段的确认后，把拥塞窗口增加至多一个MSS的数值。用这样的方法逐步增大发送方的拥塞窗口 cwnd ，可以使分组注入到网络的速率更加合理。每经过一个传输轮次，拥塞窗口 cwnd 就加倍。
  为了防止拥塞窗口cwnd增长过大引起网络拥塞，还需要设置一个慢开始门限ssthresh状态变量（如何设置ssthresh）。慢开始门限ssthresh的用法如下：
1.当 cwnd < ssthresh 时，使用上述的慢开始算法。
2.当 cwnd > ssthresh 时，停止使用慢开始算法而改用拥塞避免算法。
3.当 cwnd = ssthresh 时，既可使用慢开始算法，也可使用拥塞控制避免算法。
  **拥塞避免算法**：让拥塞窗口cwnd缓慢地增大，即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1，而不是加倍。这样拥塞窗口cwnd按线性规律缓慢增长，比慢开始算法的拥塞窗口增长速率缓慢得多。
  无论在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞（其根据就是没有收到确认），就要把慢开始门限ssthresh设置为出现拥塞时的发送方窗口值的一半（但不能小于2）。然后把拥塞窗口cwnd重新设置为1，执行慢开始算法。
  **快重传算法**：首先要求接收方每收到一个失序的报文段后就立即发出重复确认（为的是使发送方及早知道有报文段没有到达对方）而不要等到自己发送数据时才进行捎带确认。(如果当发送端接收到**三个重复的确认ACK**时，则断定分组丢失，立即重传丢失的报文段，而不必等待重传计时器超时)
  **快恢复算法**：1.当发送方连续收到三个重复确认，就执行“乘法减小”算法，把慢开始门限ssthresh减半。这是为了预防网络发生拥塞。请注意：接下去不执行慢开始算法。
2.由于发送方现在认为网络很可能没有发生拥塞，因此与慢开始不同之处是现在不执行慢开始算法（即拥塞窗口cwnd现在不设置为1），而是把cwnd值设置为慢开始门限ssthresh减半后的数值，然后开始执行拥塞避免算法（“加法增大”），使拥塞窗口缓慢地线性增大。

- **TCP中connect()、listen()和accept()**
  https://blog.csdn.net/tennysonsky/article/details/45621341

- **TCP和HTTP的头部阻塞**
https://www.cnblogs.com/mengff/p/12724641.html
1. TCP队头阻塞
TCP数据包是有序传输，中间一个数据包丢失，会等待该数据包重传，造成后面的数据包的阻塞。
2. HTTP队头阻塞
会造成队头阻塞，前一个响应未及时返回，后面的响应被阻塞。

解决http队头阻塞的方法：
1. 并发TCP连接（浏览器一个域名采用6-8个TCP连接，并发HTTP请求）
2. 域名分片（多个域名，可以建立更多的TCP连接，从而提高HTTP请求的并发）
3. HTTP2方式
    http2使用一个域名单一TCP连接发送请求，请求包被二进制分帧，不同请求可以互相穿插，避免了http层面的请求队头阻塞。
    但是不能避免TCP层面的队头阻塞。

- **如何解决TCP粘包？**
1、发送端给每个数据包添加包首部，首部中应该至少包含数据包的长度，这样接收端在接收到数据后，通过读取包首部的长度字段，便知道每一个数据包的实际长度了。
2、发送端将每个数据包封装为固定长度（不够的可以通过补0填充），这样接收端每次从接收缓冲区中读取固定长度的数据就自然而然的把每个数据包拆分开来。
3、可以在数据包之间设置边界，如添加特殊符号，这样，接收端通过这个边界就可以将不同的数据包拆分开。

- **HTTP有哪些函数？**


- **HTTP和HTTPS的区别**
  HTTP协议传输的数据都是未加密的，也就是明文的，因此使用HTTP协议传输隐私信息非常不安全，为了保证这些隐私数据能加密传输，于是网景公司设计了SSL（Secure Sockets Layer）协议用于对HTTP协议传输的数据进行加密，从而就诞生了HTTPS。简单来说，HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，要比http协议安全。
  HTTPS和HTTP的区别主要如下：
　　1、https协议需要到ca申请证书，一般免费证书较少，因而需要一定费用。
　　2、http是超文本传输协议，信息是明文传输，https则是具有安全性的ssl加密传输协议。
　　3、http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。
　　4、http的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。
  HTTPS的优点：
   （1）使用HTTPS协议可认证用户和服务器，确保数据发送到正确的客户机和服务器；
   （2）HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，要比http协议安全，可防止数据在传输过程中不被窃取、改变，确保数据的完整性。
   （3）HTTPS是现行架构下最安全的解决方案，虽然不是绝对安全，但它大幅增加了中间人攻击的成本。
  HTTPS的缺点：
   （1）HTTPS协议握手阶段比较费时，会使页面的加载时间延长近50%，增加10%到20%的耗电；
   （2）HTTPS连接缓存不如HTTP高效，会增加数据开销和功耗，甚至已有的安全措施也会因此而受到影响；
   （3）SSL证书需要钱，功能越强大的证书费用越高，个人网站、小网站没有必要一般不会用。
   （4）SSL证书通常需要绑定IP，不能在同一IP上绑定多个域名，IPv4资源不可能支撑这个消耗。
   （5）HTTPS协议的加密范围也比较有限，在黑客攻击、拒绝服务攻击、服务器劫持等方面几乎起不到什么作用。最关键的，SSL证书的信用链体系并不安全，特别是在某些国家可以控制CA根证书的情况下，中间人攻击一样可行。
  
- **HTTP 请求方法**

- **列举 HTTP 状态码**
  1XX：通知
  2XX：成功   200("OK")
  3XX：重定向   304("Not Modified")
  4XX：客户端错误   400("Bad Request")   403("Forbidden")    404("Not Found")
  5XX：服务端错误   502("Bad Gateway")
  
- **HTTP和TCP中的keep_alive**
  HTTP中叫做“Keep-Alive”, 是为了连接复用，即减少多余的TCP请求
  TCP中叫做“KeepAlive”，是为了保证连接是健康的，如即时通讯技术需要用到
  
- **HTTP的连接复用/多路复用**
  https://www.cnblogs.com/zl1991/p/12918229.html 对比了1.1、2的连接复用
  https://segmentfault.com/a/1190000011172823 详细说了HTTP2的多路复用
  简而言之，1.1的keepalive只是复用了单个连接，还是有顺序问题，2里是整个整个域名下的都基于一个链接，采用帧和流进行传输，通过序列标识在传输后进行拼装。
  
- **post和get有什么区别？**
  
  分点回答
  
  1.作用上：get用于获取资源，post用于传输实体主体
  2.参数上：get和post都可以使用额外的参数，但是get的参数是以查询字符串的形式放在url中，而post的参数是放在请求体当中。url仅支持ascii码，参数中如果存在中文等其他字符都需要转码，而post不用。
  3.安全：从传输安全上来看两者都不安全，但是从改变服务器状态上来看get是安全的，而 POST 却不是。因为 POST 的目的是传送实体主体内容，这个内容可能是用户上传的表单数据，上传成功之后，服务器可能把这个数据存储到数据库中，因此状态也就发生了改变。
  4.幂等性：同样的请求被执行一次与连续执行多次的效果是一样的，服务器的状态也是一样的。get每次调用的结果是一样的，而post如果是上传表单/增加行，那么就不是幂等的。
  5.可缓存性：在对响应进行缓存的时候，get方法是可以缓存的，post大多数情况下不可缓存。
  
  1.两者都是明文传送，但是GET的URL会被放在浏览器历史和WEB服务器日志里面，POST 发完基本就木有了，从这点上说post比get安全一些。但是被抓包的情况下两者都不安全。
  2.可以重复的交互，比如取个数据，跳个页面， 用GET。不可以重复的操作， 比如创建一个条目/修改一条记录，用POST，因为POST不能被缓存，所以浏览器不会多次提交。
  3.get表达的是一种幂等的，只读的，纯粹的操作，即它除了返回结果不应该会产生其它副作用(如写数据库)，因此绝大部分get请求(通常超过90%)都直接被CDN缓存了，这能大大减少web服
  务器的负担。而post所表达的语义是非幂等的，有副作用的操作，所以必须交由web服务器处理。
  4.GET请求会显示请求指定的资源。一般来说GET方法应该只用于数据的读取，而不应当用于会产生副作用的非幂等的操作中。POST请求会 向指定资源提交数据，请求服务器进行处理，如：表单数据提交、文件上传等，请求数据会被包含在请求体中。POST方法是非幂等的方法，因为这个请求可能会创建新的资源或/和修改现有资源。
  
- **ping操作用到了哪些协议？**
  UDP ICMP ARP OSPF
  具体过程是：首先进程在应用层发起一个IP的ping请求，传输层接收到请求，将其加上udp的头部，转发到IP层，IP层根据ICMP 协议进行封装，添加源IP和目标IP封装成为数据包，然后转到链路层，链路层接收到数据包，进行封装对应的mac地址，调用ARP协议，查询ARP缓存表，没有找到则广播出去，寻找对应IP的mac地址，这个过程用到了路由的协议OSPF。
  
- **session和cookies的区别**
  1、cookie数据存放在客户的浏览器上，session数据放在服务器上。
  2、cookie不是很安全，别人可以分析存放在本地的cookie并进行cookie欺骗，考虑到安全应当使用session。
  3、session会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能，考虑到减轻服务器性能方面，应当使用cookie。
  4、单个cookie保存的数据不能超过4K，很多浏览器都限制一个站点最多保存20个cookie。
  5、可以考虑将登陆信息等重要信息存放为session，其他信息如果需要保留，可以放在cookie中。
  1，session 在服务器端，cookie 在客户端（浏览器）
  2，session 默认被存在在服务器的一个文件里（不是内存）
  3，session 的运行依赖 session id，而 session id 是存在 cookie 中的，也就是说，如果浏览器禁用了 cookie ，同时 session 也会失效（但是可以通过其它方式实现，比如在 url 中传递 session_id）
  4，session 可以放在 文件、数据库、或内存中都可以。
  5，用户验证这种场合一般会用 session

  https://www.cnblogs.com/8023-CHD/p/11067141.html
  
- **sessions**
除了可以将用户信息通过 Cookie 存储在用户浏览器中，也可以利用 Session 存储在服务器端，存储在服务器端的信息更加安全。
Session 可以存储在服务器上的文件、数据库或者内存中。也可以将 Session 存储在 Redis 这种内存型数据库中，效率会更高。
使用 Session 维护用户登录状态的过程如下：
  用户进行登录时，用户提交包含用户名和密码的表单，放入 HTTP 请求报文中；
  服务器验证该用户名和密码，如果正确则把用户信息存储到 Redis 中，它在 Redis 中的 Key 称为 Session ID；
  服务器返回的响应报文的 Set-Cookie 首部字段包含了这个 Session ID，客户端收到响应报文之后将该 Cookie 值存入浏览器中；
   客户端之后对同一个服务器进行请求时会包含该 Cookie 值，服务器收到之后提取出 Session ID，从 Redis 中取出用户信息，继续之前的业务操作。
  应该注意 Session ID 的安全性问题，不能让它被恶意攻击者轻易获取，那么就不能产生一个容易被猜到的 Session ID 值。此外，还需要经常重新生成 Session ID。在对安全性要求极高的场景下，例如转账等操作，除了使用 Session 管理用户状态之外，还需要对用户进行重新验证，比如重新输入密码，或者使用短信验证码等方式。
  
- **网络分层协议**
  应用层：TELNET，HTTP，FTP，NFS，SMTP
  表示层：加密，ASCII
  会话层：RPC，SQL
  传输层：TCP，UDP，SPX
  网络层：ARP协议，IP协议，ICMP协议，IGMP协议
  数据链路层：ATM，FDDI
  物理层：Rj45，802.3

- **地址栏输入 URL 发生了什么**
  ​        浏览器会根据你输入的 URL 地址，去查找域名是否被本地 DNS 缓存，不同浏览器对 DNS 的设置不同，如果浏览器缓存了你想访问的 URL 地址，那就直接返回 ip。如果没有缓存你的 URL 地址，浏览器就会发起系统调用来查询本机 `hosts` 文件是否有配置 ip 地址，如果找到，直接返回。如果找不到，就向网络中发起一个 DNS 查询。
  查询请求会先找到本地 DNS 服务器来查询是否包含 IP 地址，如果本地 DNS 无法查询到目标 IP 地址，就会向根域名服务器发起一个 DNS 查询。在由根域名服务器 -> 顶级域名服务器 -> 权威 DNS 服务器后，由权威服务器告诉本地服务器目标 IP 地址，再有本地 DNS 服务器告诉用户需要访问的 IP 地址。
  ​        浏览器需要和目标服务器建立 TCP 连接，需要经过三次握手的过程，具体的握手过程请参考上面的回答。
  在建立连接后，浏览器会向目标服务器发起 HTTP-GET 请求，包括其中的 URL，HTTP 1.1 后默认使用长连接，只需要一次握手即可多次传输数据。
  如果目标服务器只是一个简单的页面，就会直接返回。但是对于某些大型网站的站点，往往不会直接返回主机名所在的页面，而会直接重定向。返回的状态码就不是 200 ，而是 301,302 以 3 开头的重定向码，浏览器在获取了重定向响应后，在响应报文中 Location 项找到重定向地址，浏览器重新第一步访问即可。
  然后浏览器重新发送请求，携带新的 URL，返回状态码 200 OK，表示服务器可以响应请求，返回报文。
  
- **DNS递归查询和迭代查询**
        **简单说，主机向本地域名服务器的查询是递归查询，但本地域名服务器向根域名服务器的查询可以递归也可以迭代。**
如果根域名服务器无法告知本地 DNS 服务器下一步需要访问哪个顶级域名服务器，就会使用递归查询；如果根域名服务器能够告知 DNS 服务器下一步需要访问的顶级域名服务器，就会使用迭代查询。

- **XSS跨域脚本攻击**

  https://zhuanlan.zhihu.com/p/26177815

- **SQL注入**

  
  
  


## 操作系统OS
- **进程和线程有什么区别？线程共享哪些东西？**
  https://blog.csdn.net/qq_44900248/article/details/90051804
          进程是资源分配的最小单位，线程是CPU调度的最小单位。
          每个进程都有独立的代码和数据空间（程序上下文），程序之间的切换会有较大的开销；线程可以看做轻量级的进程，同一类线程共享代码和数据空间，每个线程都有自己独立的运行栈和程序计数器（PC），线程之间切换的开销小。
          在操作系统中能同时运行多个进程（程序）；而在同一个进程（程序）中有多个线程同时执行（通过CPU调度，在每个时间片中只有一个线程执行）
  https://blog.csdn.net/qq_44900248/article/details/90051804
          线程共享：进程代码段、进程的公有数据(利用这些共享的数据，线程很容易的实现相互之间的通讯)、进程打开的文件描述符、信号的处理器、进程的当前目录和进程用户ID与进程组ID。
  进程能访问到的内存，每个线程自然也能访问到，包括堆和栈。但是要说共享的话，每个线程拥有有各自的栈，还有寄存器中的内容，所以线程之间共享堆空间，加上部分段空间，比如代码段和全局数据空间。
  
- **进程控制块PCB**
  
  进程控制快PCB中主要包含：1.进程标识符（包括外部标识符、内部标识符） 2.处理机状态（即处理剂的上下文，包括通用寄存器、指令计数器、程序状态字PSW、用户栈指针） 3.进程调度信息 4.进程控制信息（程序和数据的地址、进程同步和通信机制、资源清单、链接指针）
  
- **多进程和多线程有什么区别？**
  |维度|多进程|多线程|总结|
  |-|-|-|-|
  |数据共享、同步|数据是分开的，共享复杂，需要用IPC；同步简单|多线程共享进程数据，共享简单；同步复杂|各有优势|
  |内存、CPU|占用内存多，切换复杂，CPU利用率低|占用内存少，切换简单，CPU利用率高|线程占优|
  |创建销毁、切换|创建销毁、切换复杂，速度慢|创建销毁、切换简单，速度快|线程占优|
  |编程调试|编程简单，调试简单|编程复杂，调试复杂|进程占优|
  |可靠性|进程间不会相互影响|一个线程挂掉将导致整个进程挂掉|进程占优|
  |分布式|适应于多核、多机分布；如果一台机器不够，扩展到多台机器比较简单|适应于多核分布|线程占优|
  https://www.php.cn/faq/416853.html
  
- **用户态和内核态有什么区别，为什么要在两者间切换？**
       内核态与用户态是操作系统的两种运行级别，当程序运行在3级特权级上时，就可以称之为运行在用户态。因为这是最低特权级，是普通的用户进程运行的特权级，大部分用户直接面对的程序都是运行在用户态；当程序运行在0级特权级上时，就可以称之为运行在内核态。
  运行在用户态下的程序不能直接访问操作系统内核数据结构和程序。当我们在系统中执行一个程序时，大部分时间是运行在用户态下的，在其需要操作系统帮助完成某些它没有权力和能力完成的工作时就会切换到内核态（比如操作硬件）。
       这两种状态的主要差别是：处于用户态执行时，进程所能访问的内存空间和对象受到限制，其所处于占有的处理器是可被抢占的。处于内核态执行时，则能访问所有的内存空间和对象，且所占有的处理器是不允许被抢占的。
  导致用户态到内核态的切换主要有三种情况：
  1.系统调用（主动）
       这是用户态进程主动要求切换到内核态的一种方式，用户态进程通过系统调用申请使用操作系统提供的服务程序完成工作。比如前例中fork()实际上就是执行了一个创建新进程的系统调用。而系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现，例如Linux的int 80h中断。用户程序通常调用库函数，由库函数再调用系统调用，因此有的库函数会使用户程序进入内核态（只要库函数中某处调用了系统调用），有的则不会。**常见的系统调用有进程创建与销毁，内存分配与释放，文件的读写，获取或设置系统的日期时间等等。**
  2.异常（被动）
       当CPU在执行运行在用户态下的程序时，发生了某些事先不可知的异常，这时会触发由当前运行进程切换到处理此异常的内核相关程序中，也就转到了内核态，比如缺页异常。
  3.外围设备的中断（被动）
       当外围设备完成用户请求的操作后，会向CPU发出相应的中断信号，这时CPU会暂停执行下一条即将要执行的指令转而去执行与中断信号对应的处理程序，如果先前执行的指令是用户态下的程序，那么这个转换的过程自然也就发生了由用户态到内核态的切换。比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作等。
  内核态与用户态之间的切换是发生在一个线程中的。
  https://blog.csdn.net/hahachenchen789/article/details/79934208
         一般现代CPU都有几种不同的指令执行级别。**在高执行级别下，代码可以执行特权指令，访问任意的物理地址，这种CPU执行级别就对应着内核态。而在相应的低级别执行状态下，代码的掌控范围会受到限制。只能在对应级别允许的范围内活动。**
<img src="pics/用户态内核态.jpeg">
https://blog.csdn.net/qq_29996285/article/details/88078906?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase
<img src="pics/特权指令.png">
  **用户态切换到内核态的唯一途径——>中断/异常/陷入**
  **内核态切换到用户态的途径——>
  
- **内核态切换**
  
  **内核态切换不一定进行上下文切换**
  
  1.非上下文切换时：CPU 从用户态切换到内核态，保存用户态的寄存器数据，执行内核代码以获取数据，将数据存储到调用者可访问的存储器或寄存器，恢复用户态的寄存器数据并返回。这里未进行完整的上下文切换，只是涉及用户态到内核态的模式切换。
  
  2.上下文切换时：在这种情况下，内核将被迫保存调用者的完整上下文，将其标记为 阻塞态，以便调度程序在该事件或数据到达之前无法运行它。这时调度程序会加载另一个就绪线程/进程的上下文。这里的内核态切换就会导致上下文切换。
  
- **设置程序状态字**
  CS寄存器的低两位决定了当前CPU的特权等级，即RPL字段。
  CS的全拼为“Code segment”，即代码段寄存器，对应于内存中的存放代码的内存区域，用来存放内存代码段区域的入口地址（段基址）。CPU在执行指令时，通过代码寄存器CS和指令指针寄存器IP（instruction Pointer）来确定要执行的下一条指令的内存地址。**CS:IP 两个寄存器指示了CPU当前要读取的指令地址，计算方式一般为CS左移4位然后加上IP寄存器，作为地址去取内容。**
  
- **几个寄存器的区别，PC、IR、CS：IP**
  指令寄存器（IR）：，用于暂存当前正在执行的指令。
  数据缓冲寄存器（DR）：
  程序计数器（PC）：用于存放下一条指令所在单元的地址的地方。
  地址寄存器（AR）：
  累加寄存器（AC）：
  状态条件寄存器（PSW）：保存由算术指令和逻辑指令运行或测试的结果建立的各种条件码内容。
  CS:IP见上条
  取指令阶段完成的任务是将现行指令从主存中取出来并送至指令寄存器中，具体的操作如下： [5] 
1、将程序计数器（PC）中的内容送至存储器地址寄存器（MAR），并送地址总线（AB）。
2、由控制单元（CU）经控制总线（CB）向存储器发读命令。
3、从主存中取出的指令通过数据总线（DB）送到存储器数据寄存器（MDR） 。
4、将MDR的内容送至指令寄存器（IR）中 。
5、将PC的内容递增，为取下一条指令做好准备 。

- **操作系统的内存管理**
https://www.jianshu.com/p/2b11639905ec?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation

- **内存查看、虚拟内存和物理内存**
  top、ps aux、cat proc/meminfo等
  top里有VIRT（虚拟内存）、RES（物理内存）、SHR（共享内存）
  ps里有VSZ（虚拟内存）、RSS（物理内存）
  
  计算机会对虚拟内存地址空间（32位为4G）分页产生页（page），对物理内存地址空间（假设256M）分页产生页帧（page frame），这个页和页帧的大小是一样大的，所以呢，在这里，虚拟内存页的个数势必要大于物理内存页帧的个数。在计算机上有一个页表（page table），就是映射虚拟内存页到物理内存页的，更确切的说是页号到页帧号的映射，而且是一对一的映射。但是问题来了，虚拟内存页的个数 > 物理内存页帧的个数，岂不是有些虚拟内存页的地址永远没有对应的物理内存地址空间？不是的，操作系统是这样处理的。操作系统有个页面失效（page fault）功能。操作系统找到一个最少使用的页帧，让他失效，并把它写入磁盘，随后把需要访问的页放到页帧中，并修改页表中的映射，这样就保证所有的页都有被调度的可能了。这就是处理虚拟内存地址到物理内存的步骤。
  虚拟内存地址由页号（与页表中的页号关联）和偏移量组成。页号就不必解释了，上面已经说了，页号对应的映射到一个页帧。那么，说说偏移量。偏移量就是我上面说的页（或者页帧）的大小，即这个页（或者页帧）到底能存多少数据。举个例子，有一个虚拟地址它的页号是4，偏移量是20，那么他的寻址过程是这样的：首先到页表中找到页号4对应的页帧号（比如为8），如果页不在内存中，则用失效机制调入页，否则把页帧号和偏移量传给MMU（CPU的内存管理单元）组成一个物理上真正存在的地址，接着就是访问物理内存中的数据了。总结起来说，虚拟内存地址的大小是与地址总线位数相关，物理内存地址的大小跟物理内存条的容量相关。
  32位操作系统能够支持的内存是2的32次方，相当于4GB内存。相对来说，64位操作系统支持的内存大小是2的64次方，相当于128GB内存，支持最大16TB虚拟内存。按照物理内存的最小两倍最大8倍计算。1位运算处理器支持256M内存。32位os＝8192M虚拟内存（也就是8G）64位=16TB(64*256M＝16384MB）＝16TB
  
- **进程间的通信方式有哪些？**
  IPC：进程间通信（interprocess communication）
  1.**管道( pipe )**：**匿名管道**和**命名管道**
  
  **匿名管道**顾名思义，它没有名字标识，匿名管道是特殊文件只存在于内存，没有存在于文件系统中，shell 命令中的「`|`」竖线就是匿名管道，通信的数据是**无格式的流并且大小受限**，通信的方式是**单向**的，数据只能在一个方向上流动，如果要双向通信，需要创建两个管道，再来**匿名管道是只能用于存在父子关系的进程间通信**，匿名管道的生命周期随着进程创建而建立，随着进程终止而消失。
  
  **命名管道**突破了匿名管道只能在亲缘关系进程间的通信限制，因为使用命名管道的前提，需要在文件系统创建一个类型为 p 的设备文件，那么毫无关系的进程就可以通过这个设备文件进行通信。另外，不管是匿名管道还是命名管道，进程写入的数据都是**缓存在内核**中，另一个进程读取数据时候自然也是从内核中获取，同时通信数据都遵循**先进先出**原则，不支持 lseek 之类的文件定位操作。
  
  2.**消息队列( message queue ) **：**消息队列**克服了管道通信的数据是无格式的字节流的问题，消息队列实际上是保存在内核的「消息链表」，消息队列的消息体是可以用户自定义的数据类型，发送数据时，会被分成一个一个独立的消息体，当然接收数据时，也要与发送方发送的消息体的数据类型保持一致，这样才能保证读取的数据是正确的。消息队列通信的速度不是最及时的，毕竟**每次数据的写入和读取都需要经过用户态与内核态之间的拷贝过程。**
  
  3.**共享内存( shared memory ) **：**共享内存**可以解决消息队列通信中用户态与内核态之间数据拷贝过程带来的开销，**它直接分配一个共享空间，每个进程都可以直接访问**，就像访问进程自己的空间一样快捷方便，不需要陷入内核态或者系统调用，大大提高了通信的速度，是**最快的进程间通信方式**。但是便捷高效的共享内存通信，**带来新的问题，多进程竞争同个共享资源会造成数据的错乱。**
  
  4.**信号量( semophore ) **：信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的**同步手段**。确保任何时刻只能有一个进程访问共享资源，这种方式就是互斥访问。**信号量不仅可以实现访问的互斥性，还可以实现进程间的同步**，信号量其实是一个计数器，表示的是资源个数，其值可以通过两个原子操作来控制，分别是 **P 操作和 V 操作**。
  
  4.**信号 ( sinal )** ：信号是进程间通信机制中**唯一的异步通信机制**，信号可以在应用进程和内核之间直接交互，内核也可以利用信号来通知用户空间的进程发生了哪些系统事件，信号事件的来源主要有硬件来源（如键盘 Cltr+C ）和软件来源（如 kill 命令），一旦有信号发生，**进程有三种方式响应信号 1. 执行默认操作、2. 捕捉信号、3. 忽略信号**。有两个信号是应用进程无法捕捉和忽略的，即 `SIGKILL` 和 `SEGSTOP`，这是为了方便我们能在任何时候结束或停止某个进程。
  
  6.**套接字( socket ) **：如果**要与不同主机的进程间通信，那么就需要 Socket 通信了**。Socket 实际上不仅用于不同的主机进程间通信，还可以用于本地主机进程间通信，可根据创建 Socket 的类型不同，分为三种常见的通信方式，一个是基于 TCP 协议的通信方式，一个是基于 UDP 协议的通信方式，一个是本地进程间通信方式。
  https://www.cnblogs.com/liugh-wait/p/8533003.html 具体介绍看这里
  
- 敖丙的文章https://mp.weixin.qq.com/s/mblyh6XrLj1bCwL0Evs-Vg

- **系统调用的实现**

  首先应用程序不会直接使用系统调用，而是通过封装API接口来实现（API调用一个或者多个系统调用来完成所需要的功能），通过软件中断来通知内核执行对应的系统调用，传入系统调用号和参数，系统调用会检查参数的合法有效性和权限，

## 数据库/mysql/Mongo
- ACID
  原子性：事务是访问和更新数据库的程序执行的最小的单元，事务中可能包含一个或多个sql语句，这些语句要么都执行，要么都不执行，是作为一个整体的。
  一致性：事务执行前后，数据库的完整性是一致的，包括主键，表的结构，字段的类型、长度，以及例如转账后A+B余额不能改变，这些都是一致性追求的东西。
  隔离性：不同的事务在并发执行的时候不能互相干扰，尤其是读和写的操作上。
  持久性：事务执行成功以后，对数据库的操作影响是持续有效的，结果是持久化的。
- 锁机制：
  锁机制的基本原理可以概括为：事务在修改数据之前，需要先获得相应的锁；获得锁之后，事务便可以修改数据；该事务操作期间，这部分数据是锁定的，其他事务如果需要修改数据，需要等待当前事务提交或回滚后释放锁。
- 读操作可能引起的问题
  （1）脏读：当前事务(A)中可以读到其他事务(B)未提交的数据（脏数据），这种现象是脏读。
  （2）不可重复读：在事务A中先后两次读取同一个数据，两次读取的结果不一样，这种现象称为不可重复读。脏读与不可重复读的区别在于：前者读到的是其他事务未提交的数据，后者读到的是其他事务已提交的数据。
  （3）幻读：在事务A中按照某个条件先后两次查询数据库，两次查询结果的条数不同，这种现象称为幻读。不可重复读与幻读的区别可以通俗的理解为：前者是数据变了，后者是数据的行数变了。
- SQL的隔离机制（从低到高）
  读未提交
  读已提交
  可重复读
  可串行化
  大多数数据库系统中，默认的隔离级别是读已提交(如Oracle)或可重复读（后文简称RR，如InnoDB）
  InnoDB解决的方法：（1）读脏：读的时候检查是否已提交，没有的话读最新数据并回滚。（2）不可重复读：第一次读的时候记录版本号，发现版本号不一致的时候同样回滚。（3）幻读：对读取的所有行都记录版本号，发现版本号不一致的时候同样回滚。
  在mongoDB里：多文档的事务操作在4.0版本开始支持，但只支持副本集，4.2开始支持分布式事务操作。
   （1）开始事务之前需要创建会话，比如python版本：
```python
  with client.start_session() as s:
　　s.start_transaction()
　　collection_one.insert_one(doc_one, session=s)
　　collection_two.insert_one(doc_two, session=s)
　　s.commit_transaction()
```
   （2）当两个(或多个)并发事务修改相同的文档时，会发生冲突。即使在尚未提交事务时，MongoDB也可以立即检测到冲突。例如：当在执行创建索引时，未加{background:1}后台创建，此时新事务将无法获取所需的锁，并且在等待参数maxTransactionLockRequestTimeoutMillis后事务终止回滚，默认值是5毫秒。
   （3）MongoDB采用的默认隔离级别是Snapshot一致性快照(特别是设置了readConcern=majority情况下，要读某行数据的历史版本时，依赖该隔离级别。)Snapshot介于Repeatable Read与Serializable之间，既避免了脏读、不可重复读、幻读，又不会因Serializable串行化降低并发性能。
   （4）**原子性**：针对多文档的事务操作，MongoDB 提供 "All or nothing" 的原子语义保证。
            **隔离性**：MongoDB 提供 snapshot 隔离级别，在事务开始创建一个 WiredTiger snapshot，然后在整个事务过程中使用这个快照提供事务读。持久性：事务使用 WriteConcern {j: ture} 时，MongoDB 一定会保证事务日志提交才返回，即使发生 crash，MongoDB 也能根据事务日志来恢复;而如果没有指定 {j: true} 级别，即使事务提交成功了，在 crash recovery 之后，事务的也可能被回滚掉。
   （5）mongo在读写的时候会加锁，所以对同文档的修改是不会出现问题的，但是mongodb在分片和复制集的时候会产生脏读，后面在研究。

- Mongo的语句优化


- CAP
一致性（C）：在分布式系统中的所有数据备份，在同一时刻是否同样的值。
可用性（A）：在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求。
分区容忍性（P）：集群中的某些节点在无法联系后，集群整体是否还能继续进行服务。
在分布式存储系统中，最多只能实现上面的两点。

- MongoDB ACID 多文档事务支持
|事务属性|支持程度|
|-|-|
|Atomocity 原子性|单表单文档 ： 1.x 就支持<br/>复制集多表多行：4.0 复制集<br/>分片集群多表多行4.2|
|Consistency 一致性|writeConcern, readConcern (3.2)|
|Isolation 隔离性|readConcern (3.2)|
|Durability 持久性|Journal and Replication|
- 什么是 writeConcern ？
  writeConcern 决定一个写操作落到多少个节点上才算成功。writeConcern 的取值包括：
• 0：发起写操作，不关心是否成功；
• 1~集群最大数据节点数：写操作需要被复制到指定节点数才算成功；默认是1。
• majority：写操作需要被复制到大多数节点上才算成功。
• all：写入所有节点才算成功。
发起写操作的程序将阻塞到写操作到达指定的节点数为止
  
- j:true
  writeConcern 可以决定写操作到达多少个节点才算成功，journal 则定义如何才算成
功。取值包括：
• true: 写操作落到 journal 文件中才算成功；
• false: 写操作到达内存即算作成功。
数据库数据写入的顺序  数据在内存中先写日志文件（journal 中落地持久化日志文件），再写数据文件
  
- readPreference与readConcern
  readPreference 设置 分布式数据库从哪里读
  readConcern 什么样的数据可以读
  
- Mongo的切片

- Mongo索引
  Mongo用的B树，mysql用的是B+树，具体https://www.cnblogs.com/rjzheng/p/12316685.html

除了普通索引之外，MongoDB 支持的类型还包括：
 · 哈希(HASH)索引，哈希是另一种快速检索的数据结构，MongoDB 的 HASH 类型分片键会使用哈希索引。
 · 地理空间索引，用于支持快速的地理空间查询，如寻找附近1公里的商家。
 · 文本索引，用于支持快速的全文检索
 · 模糊索引(Wildcard Index)，一种基于匹配规则的灵活式索引，在4.2版本开始引入。

- **分布式事务**

两阶段提交协议（XA）：XA用于在全局事务中协调多个资源的机制。TM和RM之间采取两阶段提交的方案来解决一致性问题。两节点提交需要一个协调者（TM）来掌控所有参与者（RM）节点的操作结果并且指引这些节点是否需要最终提交。两阶段提交的局限在于协议成本，准备阶段的持久成本，全局事务状态的持久成本，潜在故障点多带来的脆弱性，准备后，提交前的故障引发一系列隔离与恢复难题。

- InnoDB和MyISAM的区别
  事务：InnoDB 是事务型的，可以使用 Commit 和 Rollback 语句。
  并发：MyISAM 只支持表级锁，而 InnoDB 还支持行级锁。
  外键：InnoDB 支持外键。
  备份：InnoDB 支持在线热备份。
  崩溃恢复：MyISAM 崩溃后发生损坏的概率比 InnoDB 高很多，而且恢复的速度也更慢。
  其它特性：MyISAM 支持压缩表和空间数据索引。
  
- InnoDB中Binlog和redo log的区别
   redo log 是 InnoDB 引擎特有的日志，而 Server 层也有自己的日志，称为 binlog（归档日志）。
   这两种日志有以下三点不同。
1.redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。
2.redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。
3.redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。


## 面试题
- 1、假设淘宝一天有5亿条成交数据，求出销量最高的100个商品并给出算法的时间复杂度。
  第一题先用hash统计，然后维护一个堆求top100，总的时间复杂度是O(n)+O(mlg100)吧，m是不重复商品总数.....
- 2、给一列无序数组，求出中位数并给出算法的时间复杂度。
  线性算法/分治
  用分治策略（类似快排），O(n)的复杂度
- 3、输入一个整型数组，求出子数组和的最大值，并给出算法的时间复杂度。
  DP
- 4、给出10W条人和人之间的朋友关系，求出这些朋友关系中有多少个朋友圈(如A-B、 B-C、D-E、E-F， 这4对关系中存在两个朋友圈)，并给出算法的时间复杂度。
  并查集
- 5、如图所示的数字三角形，从顶部出发，在每一结点可以选择向左走或得向右走，一直走到底
层，要求找出一条路径，使路径上的值的和最大。给出算法的时间复杂度。
  DP
- 6、有一个很长二进制串，求出除以3的余数是多少，给出算法的时间复杂度。
  DFA

## Python
- **闭包？**
  简单来说，闭包的概念就是当我们在函数内定义一个函数时，这个内部函数使用了外部函数的临时变量，且外部函数的返回值是内部函数的引用时，我们称之为闭包。
- **python的装饰器是什么？**
  装饰器用来封装一个函数，并且用这样或者那样的方式来修改它的行为，增加函数可以做的事情。
```python
def a_new_decorator(a_func):

    def wrapTheFunction():
        print("I am doing some boring work before executing a_func()")

        a_func()
         
        print("I am doing some boring work after executing a_func()")

    return wrapTheFunction

def a_function_requiring_decoration():
    print("I am the function which needs some decoration to remove my foul smell")

a_function_requiring_decoration()
#outputs: "I am the function which needs some decoration to remove my foul smell"

a_function_requiring_decoration = a_new_decorator(a_function_requiring_decoration)
#now a_function_requiring_decoration is wrapped by wrapTheFunction()

a_function_requiring_decoration()
#outputs:I am doing some boring work before executing a_func()
#        I am the function which needs some decoration to remove my foul smell
#        I am doing some boring work after executing a_func()
```
- **都说python比其他语言慢，为什么？怎么加快**
  编译语言和解释语言，从本质上来说就是完全不同的：编译语言能最终直接对应到机器码。C/C++是典型的编译语言。有JIT的解释语言和真·解释语言，从实现上来说完全不同：JIT可以通过技术手段将解释语言进行重新整理，变成适合编译的结构，然后再转换成机器码，得到接近于编译语言的性能。典型的JIT语言是Java、Javascript同样是解释语言，抽象层次低的语言会比抽象层次高的语言实现起来更容易，运行也更快，典型的如Lua。
  Python比 C++执行效率低是多种原因交织在一起的原因。我觉得关键问题是 动态类型、解释执行、虚拟机、GIL这四个方面的问题：1、为了支持动态类型，Python对象加入了很多抽象，执行的时候要不断的判断数据类型，带来很大的开销2、python代码由解释器逐条解释执行（interactive model）或每次执行都要先翻译再运行，运行效率大大降低。3、虚拟机带来间接开销，而且Python的虚拟机cpython性也不如jvm。4、GIL带来的伪多线程问题，所以python大力发展协程和异步语法。C++是直接编译成可执行二进制代码，有系统原生的进程、线程加持，没有动态类型的开销，没有虚拟机的开销，如果没有使用虚函数、虚继承等特性，C++代码能获得C语言一样的效率。同样运行在虚拟机上的java，编译成字节码再执行，效率就高多了，重IO的场景与C++相比也不遑多让。Python的另一个解释器PyPy引入了JIT、多线程等，执行效率蹭蹭的就上去了。
  Python 主要几个性能损失点：大量使用堆分配内存、无处不在的引用计数、支持各种动态类型、对象行为自定义导致的各种函数调用、当然还有解释执行。
  一篇比较好的博客https://blog.csdn.net/june_young_fan/article/details/83547759 里面几方面原因都说到了。**既有GIL的原因，也有动态解释语言的原因，也有编译器的原因。**
  PyPy使用了明显快于CPython的JIT。JIT本身不会使执行速度加快，因为它执行的仍然是同样的字节码序列。但是JIT会允许在运行时进行优化。
  https://www.jianshu.com/p/7792a332ca98 
  
- **Python里\*和\*\*有什么区别?**
  https://blog.csdn.net/yilovexing/article/details/80577510
  运算符上：\*乘法，\*\*乘方
  可变参数上：\*无参数名字的可变参数列表（tuple），\*\*有参数名字的可变参数列表（dict）
  \*还有解包的功能
- **__开头的函数你知道哪些**
  以下划线开头的标识符是有特殊意义的。以单下划线开头 _foo 的代表不能直接访问的类属性，需通过类提供的接口进行访问，不能用 from xxx import * 而导入。
  以双下划线开头的 __foo 代表类的私有成员，以双下划线开头和结尾的 __foo__ 代表 Python 里特殊方法专用的标识，如 __init__() 代表类的构造函数。
  __dir__()：列出对象的所有属性（方法）名
  __dict__：字典形式保存的对象里面的所有属性以及属性值（包括私有属性）
  __bases__：查看继承的父类
  __str__：默认存储对象的地址
  __hash__：哈希算法
  __call__：类名可直接调用内部的这个函数
  https://blog.csdn.net/weixin_44252534/article/details/86485197 详细的这里有
  
- **全局解释器锁Global Interpreter Lock(GIL)**
  https://www.jianshu.com/p/dbc3f3ceb5a7
  引用计数器需要保护,当多个线程同时修改这个值时,可能会导致内存泄漏;SO,我们使用锁来解决这个问题,可有时会添加多个锁来解决,这就会导致另个问题,死锁;
  为了避免内存泄漏和死锁问题,CPython使用了单锁,即全局解释器锁(GIL),即执行Python字节码都需要获取GIL,这可以防止死锁,但它有效地使任何受CPU限制的Python程序都是单线程.
  GIL对I/O绑定多线程程序的性能影响不大,因为线程在等待I/O时共享锁.
  GIL对计算型绑定多线程程序有影响,例如: 使用线程处理部分图像的程序,不仅会因锁定而成为单线程,而且还会看到执行时间的增加,这种增加是由锁的获取和释放开销的结果.
  
- **协程的概念**
  https://www.cnblogs.com/russellyoung/p/python-zhi-xie-cheng.html 这里说的比较详细
  可以理解协程就是比线程更小的单位，可以使得GIL下在单线程内实现任务切换，提高速度，但是没有IO的情况下会徒增时间。
  
- **如果我退出了当前程序，是不是其他人就访问不到了？你确定吗？如果要的话要怎么办？**
  **目前猜是问的守护进程**
  https://www.cnblogs.com/liwei0526vip/p/8972384.html

- **python的gc机制**
  https://blog.csdn.net/xiongchengluo1129/article/details/80462651 （下面的引用链接有源码解读）
  1.引用计数
  优点：（1）高效（2）运行期没有停顿 可以类比一下Ruby的垃圾回收机制，也就是 实时性：一旦没有引用，内存就直接释放了。不用像其他机制等到特定时机。实时性还带来一个好处：处理回收内存的时间分摊到了平时。（3）对象有确定的生命周期（4）易于实现
  缺点：（1）维护引用计数消耗资源，维护引用计数的次数和引用赋值成正比，而不像mark and sweep等基本与回收的内存数量有关。（2）无法解决循环引用的问题。A和B相互引用而再没有外部引用A与B中的任何一个，它们的引用计数都为1，但显然应该被回收。
  2.标记-清除（Mark—Sweep）：主要解决循环引用的问题。对象之间通过引用（指针）连在一起，构成一个有向图，对象构成这个有向图的节点，而引用关系构成这个有向图的边。从根对象（root object）出发，沿着有向边遍历对象，可达的（reachable）对象标记为活动对象，不可达的对象就是要被清除的非活动对象。根对象就是全局变量、调用栈、寄存器。
  标记清除算法作为Python的辅助垃圾收集技术主要处理的是一些容器对象，比如list、dict、tuple，instance等，因为对于字符串、数值对象是不可能造成循环引用问题。Python使用一个双向链表将这些容器对象组织起来。不过，这种简单粗暴的标记清除算法也有明显的**缺点：清除非活动的对象前它必须顺序扫描整个堆内存，哪怕只剩下小部分活动对象也要扫描所有对象**。 

  

https://zhuanlan.zhihu.com/p/142147342 python的一些面试题


## 其他

- **快速排序**
  一个比较简单的快排的python写法（不建议）
```python
	def quickSort(array):
    if len(array) < 2:
        return array
    else:
        pivot = array[0]
        less = [i for i in array[1:] if i < pivot]
        greater = [j for j in array[1:] if j >= pivot]
        return quickSort(less) + [pivot] + quickSort(greater)
print quickSort([1,5,5,2,6,9,3])
```
正确写法（pivot取首部）

```python
def quickSort(nums, left, right):
    if left >= right:
        return
    i, j = left, right
    pivot = nums[left]
    while i < j:
        while nums[j] >= pivot and i < j:
            j -= 1
        while nums[i] <= pivot and i < j:
            i += 1
        if i < j:
            temp = nums[i]
            nums[i] = nums[j]
            nums[j] = temp
    temp = nums[left]
    nums[left] = nums[i]
    nums[i] = temp
    quickSort(nums, left, i - 1)
    quickSort(nums, i + 1, right)
```

```Java
public static void qucikSort(int[] arr,int left,int right){
        int i,j,pivot,temp;
        if(left>=right){
            return;
        }
        i=left;
        j=right;
        pivot=arr[left];
        while(i<j){
            while(i<j&&arr[j]>pivot){
                j--;
            }
            while(i<j&&arr[i]<pivot){
                i++;
            }
            if(i<j){
                temp=arr[i];
                arr[i]=arr[j];
                arr[j]=temp;
            }
            temp=arr[i];
            arr[i]=pivot;
            pivot=temp;

        }
        qucikSort(arr,left,i-1);
        qucikSort(arr,i+1,right);
    }
```
- **快速排序的优化**
  a. 优化枢轴点选取方式：即尽量选取较为中间大小的元素作为中枢点pivot，例如
   1.三数取中，选三个数（也可以更多）作为样本，取其中位数作为枢轴点，这样划分更均衡。
   2.直接找到中位数作枢轴点。选取中位数的可以在 O(n)时间内完成。
  b. 三分序列
   经典快排分成大于枢轴和小于枢轴两堆，而遇到等于枢轴的元素时根据具体实现不同全部在枢轴左边或右边。所以才会导致元素全部一样时退化到O（n^2）的问题。所以改进方法是在分区的时候，将序列分为 3 堆，一堆小于中轴元素，一堆等于中轴元素，一堆大于中轴元。次递归调用快速排序的时候，只需对小于和大于中轴元素的两堆数据进行排序。
  c. 小数组变插入排序
   为什么当数据很少时要用插排呢？其实有时候当规模较小时分治算法效率是不如普通算法的。插入排序中包含的常数因子较小，使得当 n 较小时,算法运行得更快。因此,当数组递归子序列的规模在一个固定阀值(阀值指数组的最大长度)以下时,采用插入排序对子序列进行排序,能够缩短排序时间。一般阀值选取在 7到10 个元素，我记得Java里的Arrays.sort()方法好像是用的7个元素（当元素是基本类型时sort用的是优化版快排，当元素是对象时用的是归并排序，因为对于对象需要稳定性）。
  d. 多线程快排
   这块不了解，就明说吧
- **排序相关的面试题**
  https://www.jianshu.com/p/591bfbbd2661
- **排序算法的比较**
排序法 | 最差时间分析 | 平均时间复杂度 | 稳定度 | 空间复杂度
-|-|-|-|-
冒泡排序 | `O(n^2)` |` O(n^2)` | 稳定 | `O(1)`
快速排序 | `O(n^2) `| `O(n*log2n) `| 不稳定 | `O(log2n)~O(n)`
选择排序 |` O(n^2)` | `O(n^2)` | 不稳定 |` O(1)`
二叉树排序 |` O(n^2) `| `O(n*log2n) `| 不一定 |`O(n)`
插入排序 |` O(n^2)` |` O(n^2)` | 稳定 | `O(1)`
堆排序 |` O(n*log2n)` | `O(n*log2n)` | 不稳定 | `O(1)`
希尔排序 |` O(n^2) `| `O(n^1.3) `| 不稳定 | `O(1)`
归并排序 | `O(n*log2n) ` |` O(n*log2n)` | 稳定 |`O(1)`
- 


## 数据结构
- B树和B+树的区别？B+树又什么优点

## 海量数据TopK
https://blog.csdn.net/v_JULY_v/article/details/6279498
去重+topk -> bitmap
top次数 -> hash分治再归并

快排思想（随机选择法）
https://www.sohu.com/a/255145095_178889

## 短链接网址原理

https://soulmachine.gitbooks.io/system-design/content/cn/tinyurl.html