# 项目细节
## 项目介绍一下，有什么亮点？


## 爬虫用什么框架？遇到哪些问题？怎么解决？如果是动态，要怎么解决？反爬呢？
- 爬虫用的是beautiful4框架，主要用的是lxml解析器。
- 我们在搜索引擎搜索和网站单独适配中选择了网站单独适配这个策略。主要原因是搜索引擎收录的不够全面，网站来源不好控制（虽然可以使用站内搜索），我们的主要目标是主流新闻媒体，最终选择每个网站单独适配，做定向爬虫，在网站自身的搜索引擎的结果里爬数据。
- 对于常规的网站还是很容易的，搜索关键词和页码之类的信息url都包含很容易，有一些动态加载的网页很麻烦，比如点击更多才会加载，或者是翻页是直接动态变化的，这类需要去抓请求然后解析post字段的参数，模仿后去请求。更麻烦的还需要带缓存。还有的网站是没有自己的搜索功能，依赖谷歌的站内搜索，这种因为谷歌反爬不好解决，我们尝试了以后放弃了这部分。还有的需要付费，这类的也暂时无解。如果需要点击或者滚动之类的模仿操作还需要用selenium解决，不过新闻网站其实还不需要。再反爬的情况可以用IP代理池解决，我们找了免费的代理池但是后来也并没有用上（主要是也不太好用）。
## 数据怎么获取的？爬了哪些网站？爬了哪些字段？存在哪里？怎么管理？
- 22个国家57个网站。爬了XXX（省略）字段。存在了MongoDB和neo4j里，数据管理主要用的是Robo3T来操作。
## 数据规模现在有多大？以后增长之后要怎么处理？增量怎么实现？

## 项目的功能部分做了哪些，使用的什么算法，最后怎么展示可视化的？

## 翻译api优化是啥意思，非法数据指的是哪些，异常数据是哪些，怎么叫异常，修复是修复什么，怎么做？

## 相似度用的是什么算法，准确率如何，其他功能的部分你了解吗，怎么做的你知道吗？

## 主题句怎么提取的
- 用的sumy包来进行主题句提取，先用nltk进行分句，再调用sumy包里的各种算法比如lsa，textrank，lexrank等，因为这些算法各有优劣，我们就采用众数的方式，取几个算法提取出来总次数最多的几句作为主题句。谷歌最近发布了一个叫“天马”的小训练集就可以主题提取的模型，但是现在还没有时间去看。

## 毕设那个，有没有一个实际的效果的数值，比如准确度之类的？你指的模式、字段是什么意思？

## 数据库为什么选择了MongoDB？
- 新闻数据是非结构化的，每个新闻网站的字段不尽相同，结构不明确，且有些字段会很稀疏（例如author、lead等字段），用关系型数据库存储会造成大量空间的浪费，而MongoDB分结构灵活，适合存储非结构化的数据。
- 我们使用的数据以正文为主，属于文档型数据，MongoDB适合文档型存储。
- 我们的新闻数据以存储和查询为主，用于内容管理和数据分析（包含机器学习模型处理），基本不涉及事务操作，对事务一致性要求不高，不需要用MySQL这类强事务型支持的数据库，MongoDB会在内存中缓存热数据，查询效率高。
- 计划做成大数据方向，未来数据会越来越大，MySQL在大数据下性能会出现明显下降，而MongoDB是分布式场景下性能较好的数据库，内建了sharding、很多数据分片的特性，容易水平扩展，自带高可用，自动主从切换，更适应大数据量增长的需求。